{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils;\n",
    "from utils import *\n",
    "import time\n",
    "from __future__ import division, print_function\n",
    "import sys\n",
    "sys.path.insert(1, '/home/mh/opencv-master/build/lib/python3')\n",
    "import cv2\n",
    "import glob\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "data_path = '/home/mh/ws/fish_challenge/input/'\n",
    "model_path = '/home/mh/ws/fish_challenge/input/models/'\n",
    "batch_size=32\n",
    "\n",
    "batches = get_batches(data_path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(data_path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_filenames = get_batches(data_path+'test', batch_size=batch_size).filenames\n",
    "\n",
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(data_path)\n",
    "\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]\n",
    "\n",
    "## Load data.\n",
    "trn_data = load_array(model_path+'train_data.bc')\n",
    "val_data = load_array(model_path+'valid_data.bc')\n",
    "trn_labels = load_array(model_path+'trn_labels.bc')\n",
    "val_labels = load_array(model_path+'val_labels.bc')\n",
    "val_labels = onehot(val_labels)\n",
    "trn_labels = onehot(trn_labels)\n",
    "\n",
    "test_data = load_array(model_path+'test_data.bc')\n",
    "test_data = test_data.transpose((0,3,1,2))\n",
    "\n",
    "from vgg16bn import Vgg16BN\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3),\n",
    "       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.load_weights(data_path+'results/ft1.h5')\n",
    "\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)\n",
    "conv_model = Sequential(conv_layers)\n",
    "#load features\n",
    "conv_feat = load_array(data_path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(data_path+'results/conv_val_feat.dat')\n",
    "conv_test_feat = load_array(data_path+'results/conv_test_feat.dat')\n",
    "\n",
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/4),\n",
    "        Flatten(),\n",
    "        Dense(512,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(512,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(8,activation='softmax')\n",
    "    ]\n",
    "\n",
    "p=0.6\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bn_model.load_weights(data_path+'models/conv_512_6.h5')\n",
    "\n",
    "import ujson as json\n",
    "anno_classes = ['alb', 'bet', 'dol', 'lag', 'other', 'shark', 'yft']\n",
    "bb_json = {}\n",
    "for c in anno_classes:\n",
    "    j = json.load(open('{}/boxes/{}.json'.format(data_path, c), 'r'))\n",
    "    for l in j:\n",
    "        if 'annotations' in l.keys() and len(l['annotations'])>0:\n",
    "            bb_json[l['filename'].split('/')[-1]] = sorted(\n",
    "                l['annotations'], key=lambda x: x['height']*x['width'])[-1]\n",
    "\n",
    "sizes = [PIL.Image.open(data_path+'train/'+f).size for f in filenames]\n",
    "id2size = list(set(sizes))\n",
    "size2id = {o:i for i,o in enumerate(id2size)}\n",
    "raw_val_sizes = [PIL.Image.open(data_path+'valid/'+f).size for f in val_filenames]\n",
    "\n",
    "file2idx = {o:i for i,o in enumerate(raw_filenames)}\n",
    "val_file2idx = {o:i for i,o in enumerate(raw_val_filenames)}\n",
    "empty_bbox = {'height': 0., 'width': 0., 'x': 0., 'y': 0.}\n",
    "for f in raw_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "for f in raw_val_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "\n",
    "bb_params = ['height', 'width', 'x', 'y']\n",
    "def convert_bb(bb, size):\n",
    "    bb = [bb[p] for p in bb_params]\n",
    "    bb[0]= bb[0]*(224/size[0])\n",
    "    bb[1]= bb[1]*(224/size[1])\n",
    "    bb[2]= max(bb[2]*(224/size[0]),0)\n",
    "    bb[3]= max(bb[3]*(224/size[1]),0)\n",
    "    return bb\n",
    "val_bbox = np.stack([convert_bb(bb_json[f], s) \n",
    "                   for f,s in zip(raw_val_filenames, raw_val_sizes)]).astype(np.float32)\n",
    "\n",
    "trn_bbox = np.stack([convert_bb(bb_json[f],s) for f,s in zip(raw_filenames, sizes)]).astype(np.float32)\n",
    "\n",
    "# val = get_data(data_path+'valid', (360,640))\n",
    "\n",
    "def create_rect(bb, color='red'):\n",
    "    plot.Rectangle((bb[3],bb[2]),bb[1],bb[0],color=color, fill=False, lw=3)\n",
    "    \n",
    "def show_bb(i):\n",
    "    bb = val_bbox[i]\n",
    "    print(cv2.resize(val,(360,640)).shape)\n",
    "    img = val[i]\n",
    "    plot(img)\n",
    "    plt.gca().add_patch(create_rect(bb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "x = MaxPooling2D()(inp)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(p/4)(x)\n",
    "x = Flatten()(x)\n",
    "i = x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p/2)(x)\n",
    "x_bb = Dense(4, name='bb')(x)\n",
    "x_class = Dense(8, activation='softmax', name='class')(x)\n",
    "\n",
    "model = Model([inp], [x_bb, x_class])\n",
    "model.compile(Adam(lr=0.001), loss=['mse', 'categorical_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[.001, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 512, 14, 14)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 512, 14, 14)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 512, 7, 7)     0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 512, 7, 7)     2048        maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 512, 7, 7)     0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 25088)         0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 512)           12845568    flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 512)           2048        dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 512)           0           batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 512)           262656      dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 512)           2048        dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 512)           0           batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "bb (Dense)                       (None, 4)             2052        dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "class (Dense)                    (None, 8)             4104        dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 13,120,524\n",
      "Trainable params: 13,117,452\n",
      "Non-trainable params: 3,072\n",
      "____________________________________________________________________________________________________\n",
      "[  9.9770e-01   9.5691e-05   6.9905e-05   1.3177e-05   6.2169e-05   1.9886e-03   3.6950e-05\n",
      "   3.0624e-05]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(data_path+'models/bn_anno_24epochs.h5')\n",
    "\n",
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/7, mx)\n",
    "\n",
    "print(conv_test_feat.shape)\n",
    "model.summary()\n",
    "boxes, preds = model.predict(conv_test_feat, batch_size=8)\n",
    "print(preds[0])\n",
    "\n",
    "# preds = np.array(preds)\n",
    "preds = do_clip(preds,0.83)\n",
    "# subm = preds\n",
    "\n",
    "subm_name = data_path+'results/subm_24epoch_bb_clipping_0.83.gz'\n",
    "\n",
    "# classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "submission = pd.DataFrame(preds, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.head()\n",
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Merge Models\n",
    "def merge_several_folds_mean(data, num):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, num):\n",
    "        a += np.array(data[i])\n",
    "    a /= num\n",
    "    return a.tolist()\n",
    "\n",
    "yfull_test=[]\n",
    "yfull_test.append(preds)\n",
    "preds_bn_vgg = bn_model.predict(conv_test_feat, batch_size=32)\n",
    "yfull_test.append(preds_bn_vgg)\n",
    "\n",
    "merged_preds = merge_several_folds_mean(yfull_test, 2)\n",
    "\n",
    "# preds = np.array(preds)\n",
    "preds = do_clip(merged_preds,0.84)\n",
    "# subm = preds\n",
    "\n",
    "subm_name = data_path+'results/subm_24epoch_bb+bn_clipping_0.83.gz'\n",
    "\n",
    "# classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "submission = pd.DataFrame(preds, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.head()\n",
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
