{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 9 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Found 3277 images belonging to 9 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils;\n",
    "from utils import *\n",
    "import time\n",
    "from __future__ import division, print_function\n",
    "import sys\n",
    "sys.path.insert(1, '/home/mh/opencv-master/build/lib/python3')\n",
    "import cv2\n",
    "import glob\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "data_path = '/home/mh/ws/fish_challenge/input/'\n",
    "model_path = '/home/mh/ws/fish_challenge/input/models/'\n",
    "batch_size=32\n",
    "\n",
    "batches = get_batches(data_path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(data_path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_filenames = get_batches(data_path+'test', batch_size=batch_size).filenames\n",
    "\n",
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(data_path)\n",
    "\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]\n",
    "\n",
    "## Load data.\n",
    "trn_data = load_array(model_path+'train_data.bc')\n",
    "val_data = load_array(model_path+'valid_data.bc')\n",
    "trn_labels = load_array(model_path+'trn_labels.bc')\n",
    "val_labels = load_array(model_path+'val_labels.bc')\n",
    "val_labels = onehot(val_labels)\n",
    "trn_labels = onehot(trn_labels)\n",
    "\n",
    "test_data = load_array(model_path+'test_data.bc')\n",
    "test_data = test_data.transpose((0,3,1,2))\n",
    "\n",
    "from vgg16bn import Vgg16BN\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3),\n",
    "       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.load_weights(data_path+'results/ft1.h5')\n",
    "\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)\n",
    "conv_model = Sequential(conv_layers)\n",
    "#load features\n",
    "conv_feat = load_array(data_path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(data_path+'results/conv_val_feat.dat')\n",
    "conv_test_feat = load_array(data_path+'results/conv_test_feat.dat')\n",
    "\n",
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/4),\n",
    "        Flatten(),\n",
    "        Dense(512,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(512,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(8,activation='softmax')\n",
    "    ]\n",
    "\n",
    "p=0.6\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bn_model.load_weights(data_path+'models/conv_512_6.h5')\n",
    "\n",
    "import ujson as json\n",
    "anno_classes = ['alb', 'bet', 'dol', 'lag', 'other', 'shark', 'yft']\n",
    "bb_json = {}\n",
    "for c in anno_classes:\n",
    "    j = json.load(open('{}/boxes/{}.json'.format(data_path, c), 'r'))\n",
    "    for l in j:\n",
    "        if 'annotations' in l.keys() and len(l['annotations'])>0:\n",
    "            bb_json[l['filename'].split('/')[-1]] = sorted(\n",
    "                l['annotations'], key=lambda x: x['height']*x['width'])[-1]\n",
    "\n",
    "sizes = [PIL.Image.open(data_path+'train/'+f).size for f in filenames]\n",
    "id2size = list(set(sizes))\n",
    "size2id = {o:i for i,o in enumerate(id2size)}\n",
    "raw_val_sizes = [PIL.Image.open(data_path+'valid/'+f).size for f in val_filenames]\n",
    "\n",
    "file2idx = {o:i for i,o in enumerate(raw_filenames)}\n",
    "val_file2idx = {o:i for i,o in enumerate(raw_val_filenames)}\n",
    "empty_bbox = {'height': 0., 'width': 0., 'x': 0., 'y': 0.}\n",
    "for f in raw_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "for f in raw_val_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "\n",
    "bb_params = ['height', 'width', 'x', 'y']\n",
    "def convert_bb(bb, size):\n",
    "    bb = [bb[p] for p in bb_params]\n",
    "    bb[0]= bb[0]*(224/size[0])\n",
    "    bb[1]= bb[1]*(224/size[1])\n",
    "    bb[2]= max(bb[2]*(224/size[0]),0)\n",
    "    bb[3]= max(bb[3]*(224/size[1]),0)\n",
    "    return bb\n",
    "val_bbox = np.stack([convert_bb(bb_json[f], s) \n",
    "                   for f,s in zip(raw_val_filenames, raw_val_sizes)]).astype(np.float32)\n",
    "\n",
    "trn_bbox = np.stack([convert_bb(bb_json[f],s) for f,s in zip(raw_filenames, sizes)]).astype(np.float32)\n",
    "\n",
    "# val = get_data(data_path+'valid', (360,640))\n",
    "\n",
    "def create_rect(bb, color='red'):\n",
    "    plot.Rectangle((bb[3],bb[2]),bb[1],bb[0],color=color, fill=False, lw=3)\n",
    "    \n",
    "def show_bb(i):\n",
    "    bb = val_bbox[i]\n",
    "    print(cv2.resize(val,(360,640)).shape)\n",
    "    img = val[i]\n",
    "    plot(img)\n",
    "    plt.gca().add_patch(create_rect(bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include ground-truth boxes in the set of candidate rois\n",
    "\n",
    "\n",
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "# input 512*14*14\n",
    "x = Convolution2D()(x)\n",
    "# output \n",
    "x_bb = Dense(4, name='bb')(x)\n",
    "x_class = Dense(8, activation='softmax', name='class')(x)\n",
    "\n",
    "rpn_model = Model([inp], [x_bb, x_class])\n",
    "rpn_model.compile(Adam(lr=0.001), loss=['mse', 'categorical_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[.001, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "x = MaxPooling2D()(inp)\n",
    "x_bb = Dense(4, name='bb')(x)\n",
    "x_class = Dense(8, activation='softmax', name='class')(x)\n",
    "\n",
    "roi_model = Model([inp], [x_bb, x_class])\n",
    "roi_model.compile(Adam(lr=0.001), loss=['mse', 'categorical_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[.001, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
